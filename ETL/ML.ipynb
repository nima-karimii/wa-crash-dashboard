{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.12 64-bit ('PythonAdv': conda)",
   "display_name": "Python 3.6.12 64-bit ('PythonAdv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2a9f4844aa4147fbc2a2f996c83b0ccc9b2f4fc4271ef1d69160988aafd4637a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime  \n",
    "from datetime import date \n",
    "import calendar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file2 = \"crash_data2.csv\"\n",
    "Crash_df = pd.read_csv(csv_file2)\n",
    "Crash_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_data=Crash_df[['CRASH_TIME_HRS','CRASH_DAYWEEK','CRASH_DATE','ACC_ID']]\n",
    "ML_data.CRASH_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add day of the week column\n",
    "def Datesplitter(Date): \n",
    "    day, month, year = (int(i) for i in Date.split('/'))     \n",
    "    dayWeek = datetime.date(year, month, day) \n",
    "    return  year%100\n",
    "\n",
    "# date = '07/10/2020'\n",
    "# print(Datesplitter(date)[2]) \n",
    "ML_data['CRASH_YEAR'] = ML_data['CRASH_DATE'].apply(Datesplitter)\n",
    "ML_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_data_groupby=ML_data.groupby(['CRASH_YEAR','CRASH_DAYWEEK','CRASH_TIME_HRS',]).agg({'CRASH_YEAR':'max','CRASH_DAYWEEK':'max' ,'CRASH_TIME_HRS':'max','ACC_ID':'count'})\n",
    "ML_Data=ML_data_groupby.reset_index(drop=True)\n",
    "ML_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_Data.ACC_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ML_Data.ACC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add day of the week column\n",
    "def ACC_Cat(number): \n",
    "    if (number < 100): return \"Safe\"\n",
    "    elif (number <200): return \"Carefull\"\n",
    "    elif (number < 350): return \"Dangerous\"\n",
    "    else: return \"Severely Dangerous\"\n",
    "# date = '07/10/2020'\n",
    "# print(Datesplitter(date)[2]) \n",
    "ML_Data['TYPE'] = ML_Data['ACC_ID'].apply(ACC_Cat)\n",
    "ML_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maping_dict={'Safe':1, 'Severely Dangerous':4, 'Carefull':2,'Dangerous':3}\n",
    "ML_Data['TYPE_NO']=ML_Data['TYPE'].map(maping_dict)\n",
    "ML_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maping_dict={'Friday':5, 'Monday':1, 'Saturday':6, 'Sunday':7, 'Thursday':4, 'Tuesday':2,'Wednesday':3}\n",
    "ML_Data['CRASH_DAYWEEK']=ML_Data['CRASH_DAYWEEK'].map(maping_dict)\n",
    "ML_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=ML_Data[['CRASH_TIME_HRS','CRASH_DAYWEEK','CRASH_YEAR']]\n",
    "yl=ML_Data.TYPE_NO\n",
    "y=ML_Data[['TYPE_NO']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape,yl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(yl)"
   ]
  },
  {
   "source": [
    "# LinearRegression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fitting our model with all of our features in X\n",
    "model.fit(X, y)\n",
    "\n",
    "score = model.score(X, y)\n",
    "print(f\"R2 Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST=[[24,5,2019]]\n",
    "LinearRegression_Predicted=model.predict(TEST)\n",
    "LinearRegression_Predicted"
   ]
  },
  {
   "source": [
    "# LogisticRegression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training Data Score: {classifier.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST=[[15,2,2020]]\n",
    "LogisticRegression_Predicted=classifier.predict(TEST)\n",
    "LogisticRegression_Predicted"
   ]
  },
  {
   "source": [
    "# Support vector machine linear classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training Data Score: {model.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [10, 50],\n",
    "              'gamma': [0.0001, 0.0005]}\n",
    "grid = GridSearchCV(model, param_grid, verbose=3)\n",
    "grid.fit(X_train, y_train)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the best parameters for this dataset\n",
    "print(grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the hypertuned model\n",
    "predictions = grid.predict(X_test)\n",
    "# Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions,\n",
    "                            target_names=[\"1\", \"2\",\"3\",\"4\"]))"
   ]
  },
  {
   "source": [
    "# Neural_networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScater model and fit it to the training data\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "len(y_train_categorical)\n",
    "y_train_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=3))\n",
    "model.add(Dense(units=100, activation='relu', input_dim=1))\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the hypertuned model\n",
    "predictions = model.predict_classes(X_test_scaled)\n",
    "# Calculate classification report\n",
    "print(y_test[:10])\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"Accident_model_trained.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Accident_model = load_model(\"Accident_model_trained.h5\")"
   ]
  },
  {
   "source": [
    "TEST=X_test[:10]\n",
    "\n",
    "TEST_scaled = X_scaler.transform(TEST)\n",
    "\n",
    "encoded_predictions = Accident_model.predict_classes(TEST)\n",
    "prediction_labels =encoded_predictions\n",
    "print(f\"Predicted classes: {prediction_labels}\")\n",
    "print(y_test[:10])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mayhem\n",
    "# mad house\n",
    "# chaotic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(units=500, activation='relu', input_dim=3))\n",
    "model1.add(Dense(units=500, activation='relu', input_dim=1))\n",
    "model1.add(Dense(units=5, activation='softmax'))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "model1.fit(\n",
    "    X_train,\n",
    "    y_train_categorical,\n",
    "    epochs=500,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "source": [
    "# Loading the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "N_model = load_model(\"N_model_trained.h5\")\n",
    "\n",
    "TEST=[[13,5,19]]\n",
    "# [14,5,15]vs[14,5,19]\n",
    "\n",
    "predictions = N_model.predict_classes(TEST)\n",
    "print(f\"Predicted classes: {predictions[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_json=[]\n",
    "Y=20\n",
    "D=5\n",
    "H=23\n",
    "Accident_model = load_model(\"N_model_trained.h5\")\n",
    "for i in range (-2 ,3):\n",
    "    y=int(Y)\n",
    "    d=int(D)\n",
    "    h=int(H)\n",
    "    Data_predictions={}\n",
    "    Time=[[h-i,d,y]]\n",
    "    Data_predictions[\"Day_time\"]={}\n",
    "    Data_predictions[\"Day_time\"][\"H\"]=h-i\n",
    "    Data_predictions[\"Day_time\"][\"D\"]=d\n",
    "    Data_predictions[\"Day_time\"][\"Y\"]=y\n",
    "    Data_predictions[\"predictions\"] = Accident_model.predict_classes(Time)[0]\n",
    "    Data_json.append(Data_predictions)\n",
    "    Data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'Day_time': {'H': 25, 'D': 5, 'Y': 20}, 'predictions': 1},\n",
       " {'Day_time': {'H': 24, 'D': 5, 'Y': 20}, 'predictions': 1},\n",
       " {'Day_time': {'H': 23, 'D': 5, 'Y': 20}, 'predictions': 1},\n",
       " {'Day_time': {'H': 22, 'D': 5, 'Y': 20}, 'predictions': 1},\n",
       " {'Day_time': {'H': 21, 'D': 5, 'Y': 20}, 'predictions': 2}]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "    Data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'H': 25, 'D': 5, 'Y': 20}"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "   Data_json[0][\"Day_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Working outside of application context.\n\nThis typically means that you attempted to use functionality that needed\nto interface with the current application object in some way. To solve\nthis, set up an application context with app.app_context().  See the\ndocumentation for more information.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-ce8b4792c1ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjsonify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonAdv\\lib\\site-packages\\flask\\json\\__init__.py\u001b[0m in \u001b[0;36mjsonify\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[0mseparators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\":\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mcurrent_app\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"JSONIFY_PRETTYPRINT_REGULAR\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcurrent_app\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[0mindent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[0mseparators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\", \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\": \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonAdv\\lib\\site-packages\\werkzeug\\local.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__members__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_current_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_current_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonAdv\\lib\\site-packages\\werkzeug\\local.py\u001b[0m in \u001b[0;36m_get_current_object\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \"\"\"\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__local\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__release_local__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__local\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonAdv\\lib\\site-packages\\flask\\globals.py\u001b[0m in \u001b[0;36m_find_app\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mtop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_app_ctx_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtop\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_app_ctx_err_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Working outside of application context.\n\nThis typically means that you attempted to use functionality that needed\nto interface with the current application object in some way. To solve\nthis, set up an application context with app.app_context().  See the\ndocumentation for more information."
     ]
    }
   ],
   "source": [
    "from flask import jsonify\n",
    "import numpy as np\n",
    "\n",
    "print(jsonify(Data_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "normalized = scaler.transform(X_train)\n",
    "inverse = scaler.inverse_transform(normalized)\n",
    "inverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=2\n",
    "b=3\n",
    "d=4\n",
    "for i in range (-2 ,2):\n",
    "    List=[[a+i,b,d]]\n",
    "    print(List)"
   ]
  },
  {
   "source": [
    "\n",
    "# BREAK TIME ! COMMING BACK :12:30"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}